# ğŸ§  RAG-Local

A fully local implementation of **Retrieval-Augmented Generation (RAG)** using **LlamaIndex**, **ChromaDB**, and **Streamlit**.

This project demonstrates how to build a **privacy-preserving, offline AI system** capable of understanding and answering questions from your own documents â€” without depending on cloud APIs or internet connectivity.

---

## ğŸ“˜ Overview

**RAG-Local** combines **retrieval** and **generation** to enhance local large language models with your own custom knowledge base.
It follows the core stages of the RAG pipeline:

1. **Document Loading** â€” Import and preprocess your local text or PDF files.
2. **Vectorization** â€” Generate embeddings using **Ollama embeddings** and store them in **ChromaDB**.
3. **Retrieval** â€” Search for semantically relevant chunks based on a user query.
4. **Generation** â€” Produce accurate, context-aware answers using a **local Ollama LLM**.

---

## âš™ï¸ Project Structure

```
RAG Local/
â”œâ”€â”€ app.py              # Streamlit web interface for user interaction
â”œâ”€â”€ index.py            # Handles document loading and vector database creation
â”œâ”€â”€ query.py            # Performs retrieval and generation using LlamaIndex
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ data/               # Directory for source documents
â”œâ”€â”€ chroma_db/          # Local vector database storage
â””â”€â”€ README.md
```

---

## ğŸ§© Features

âœ… 100% local â€” no external API keys required
âœ… Uses **LlamaIndex** for flexible RAG pipelines
âœ… Uses **Ollama** for both LLM and embedding models
âœ… Stores vectors locally in **ChromaDB**
âœ… Includes a **Streamlit** UI for simple interaction
âœ… Modular and easy to extend

---

## ğŸ’» Installation

1. **Clone this repository**

   ```bash
   git clone https://github.com/<your-username>/RAG-Local.git
   cd RAG-Local
   ```

2. **Create and activate a virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate      # On Linux/Mac
   venv\Scripts\activate         # On Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

---

## âš™ï¸ Installing Ollama and Models

To run this project, you need **Ollama** installed on your machine. Ollama provides local execution for LLMs and embedding models.

### ğŸ§© Step 1 â€” Install Ollama

Follow the installation instructions from the official site:
ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)

After installation, verify it works:

```bash
ollama --version
```

### ğŸ§  Step 2 â€” Pull Required Models

This project uses two models:

* **`qwen3:8b`** â€” Main LLM for text generation and reasoning.
* **`nomic-embed-text`** â€” Embedding model for converting text into vector representations.

Download both models locally using the commands below:

```bash
ollama pull qwen3:8b
ollama pull nomic-embed-text
```

You can verify the models are available by running:

```bash
ollama list
```

This will display a list of installed models, including `qwen3:8b` and `nomic-embed-text`.

### ğŸ§© Optional â€” Create a Custom Modelfile

If you want to configure your own model parameters or prompt templates, you can create a `Modelfile` like this:

```Dockerfile
FROM qwen3:8b
PARAMETER temperature 0.7
PARAMETER top_p 0.9
```

Then build it with:

```bash
ollama create my-qwen3 -f Modelfile
```

You can use your custom model name (e.g., `my-qwen3`) in the code.

---

## ğŸš€ Usage

### ğŸ—ï¸ Step 1 â€” Build the Knowledge Base

Run the following command to load and embed your documents:

```bash
python index.py
```

This will process files in the `data/` folder and create a vector database in `chroma_db/`.

---

### ğŸ” Step 2 â€” Ask Questions

Once the database is ready, you can query it:

```bash
python query.py
```

Type your question, and the system will retrieve the most relevant information and generate a context-aware answer using the **Ollama LLM**.

---

### ğŸ§  Step 3 â€” Run the Streamlit App

You can launch the web-based interface with:

```bash
streamlit run app.py
```

This will open a local web interface for interactive querying.

---

## ğŸ§° Tech Stack

* **Python 3.10+**
* **LlamaIndex** â€” for managing documents, retrieval, and query pipelines
* **llama-index-llms-ollama** â€” for local LLM integration
* **llama-index-embeddings-ollama** â€” for local embeddings
* **llama-index-vector-stores-chroma** â€” for ChromaDB integration
* **ChromaDB** â€” local vector store
* **Streamlit** â€” lightweight UI framework
* **Ollama** â€” local LLM and embedding model runner

---

## ğŸ§  Models Used

This project uses **Ollama** to run both the LLM and embedding models locally:

* **`qwen3:8b`** â€” A powerful and efficient open-source large language model from Alibabaâ€™s Qwen3 family, used for **text generation and reasoning**. It provides excellent performance for retrieval-augmented generation tasks while remaining efficient enough for local execution.

* **`nomic-embed-text`** â€” An embedding model from Nomic AI used to **convert text into high-dimensional vector representations**, enabling efficient semantic search and retrieval through ChromaDB.

These models are fully compatible with **Ollama**, allowing for smooth local deployment without external API dependencies.

---

## ğŸ§© Example Use Case

> You can place your research papers, notes, or project documents inside the `data/` folder.
> Then, simply ask questions like:
>
> â€œWhat are the main topics discussed in the file data.txt?â€
>
> and the system will retrieve relevant content and summarize it locally using **qwen3:8b** and **nomic-embed-text**.

---

## ğŸ Goal

This repository was created as part of my learning journey to understand and implement **Retrieval-Augmented Generation (RAG)** locally using **LlamaIndex** and **Ollama**.
It serves as a foundation for building fully local, private, and efficient AI knowledge systems.

---

## ğŸ‘¨â€ğŸ’» Author

**Taqiyudin Miftah Adn**
ğŸ“š Computer Engineering student passionate about **Artificial Intelligence** and **Information Retrieval Systems**.
